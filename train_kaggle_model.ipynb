{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phishing Site URLs Detection\n",
                "\n",
                "This notebook demonstrates how to train a machine learning model using the **Phishing Site URLs** dataset from Kaggle.\n",
                "\n",
                "Dataset: [taruntiwarihp/phishing-site-urls](https://www.kaggle.com/datasets/taruntiwarihp/phishing-site-urls)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup\n",
                "First, we need to install the `kagglehub` library to download the dataset automatically."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install kagglehub pandas scikit-learn joblib"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Global Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import kagglehub\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import glob\n",
                "import os\n",
                "import joblib\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Download and Load Dataset\n",
                "We use `kagglehub` to fetch the latest version of the dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Downloading dataset...\")\n",
                "path = kagglehub.dataset_download(\"taruntiwarihp/phishing-site-urls\")\n",
                "print(\"Path to dataset:\", path)\n",
                "\n",
                "# Find the CSV file\n",
                "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
                "if csv_files:\n",
                "    csv_path = csv_files[0]\n",
                "    print(f\"Loading data from: {csv_path}\")\n",
                "    df = pd.read_csv(csv_path)\n",
                "    print(\"\\nDataset loaded successfully!\")\n",
                "    print(f\"Shape: {df.shape}\")\n",
                "    print(df.head())\n",
                "else:\n",
                "    print(\"Error: No CSV file found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Preprocessing\n",
                "The dataset usually contains a 'URL' column and a 'Label' column. We will use the URLs as features and Labels as the target."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Inspect columns to find label and url\n",
                "print(\"Columns:\", df.columns)\n",
                "\n",
                "# Assuming standard column names, but you can adjust if needed\n",
                "url_col = 'URL'\n",
                "label_col = 'Label'\n",
                "\n",
                "# Sample Split\n",
                "X = df[url_col]\n",
                "y = df[label_col]\n",
                "\n",
                "print(f\"\\nTotal samples: {len(X)}\")\n",
                "print(\"Class distribution:\")\n",
                "print(y.value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "\n",
                "print(f\"Training samples: {len(X_train)}\")\n",
                "print(f\"Testing samples: {len(X_test)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Feature Extraction (TF-IDF)\n",
                "We use TF-IDF with a character analyzer to capture patterns in the URL string (e.g., 'http', '.com', 'secure', 'login')."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Vectorizing URLs...\")\n",
                "# Character n-grams (3-5 chars) are very effective for URLs\n",
                "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=5000, min_df=5, max_df=0.9)\n",
                "\n",
                "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
                "X_test_tfidf = vectorizer.transform(X_test)\n",
                "\n",
                "print(f\"Feature matrix shape: {X_train_tfidf.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model Training (Logistic Regression)\n",
                "Logistic Regression is chosen for its efficiency and effectiveness on high-dimensional sparse data like text/TF-IDF."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Training model...\")\n",
                "model = LogisticRegression(max_iter=1000, n_jobs=-1, solver='saga')\n",
                "model.fit(X_train_tfidf, y_train)\n",
                "print(\"Training completed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred = model.predict(X_test_tfidf)\n",
                "\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "print(f\"Accuracy: {accuracy:.4f}\")\n",
                "\n",
                "print(\"\\nConfusion Matrix:\")\n",
                "print(confusion_matrix(y_test, y_pred))\n",
                "\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Model\n",
                "Save the trained model and vectorizer for future use."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "joblib.dump(model, 'kaggle_phishing_model.joblib')\n",
                "joblib.dump(vectorizer, 'kaggle_vectorizer.joblib')\n",
                "print(\"Model saved to 'kaggle_phishing_model.joblib'\")\n",
                "print(\"Vectorizer saved to 'kaggle_vectorizer.joblib'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Test with New URLs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "samples = [\n",
                "    \"https://www.google.com\",\n",
                "    \"http://phishing-bank-login.com/secure\",\n",
                "    \"https://www.kaggle.com\",\n",
                "    \"http://192.168.1.1/login\"\n",
                "]\n",
                "\n",
                "transformed_samples = vectorizer.transform(samples)\n",
                "predictions = model.predict(transformed_samples)\n",
                "\n",
                "for url, pred in zip(samples, predictions):\n",
                "    print(f\"URL: {url} -> Prediction: {pred}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}